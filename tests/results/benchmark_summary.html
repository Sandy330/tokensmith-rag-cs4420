
<!DOCTYPE html>
<html>
<head>
    <title>TokenSmith Benchmark Results</title>
    <style>
        body { font-family: Arial, sans-serif; margin: 40px; }
        .summary { background: #f5f5f5; padding: 20px; border-radius: 8px; margin-bottom: 30px; }
        .test-result { 
            border: 1px solid #ddd; 
            margin: 10px 0; 
            padding: 15px; 
            border-radius: 5px; 
            min-width: 0;
        }
        .passed { border-left: 5px solid #4CAF50; }
        .failed { border-left: 5px solid #f44336; }
        .score { font-weight: bold; color: #2196F3; }
        .metric-grid { display: grid; grid-template-columns: repeat(auto-fit, minmax(200px, 1fr)); gap: 10px; margin: 10px 0; }
        .metric-item { background: #f9f9f9; padding: 10px; border-radius: 3px; }
        pre { 
            background: #f9f9f9; 
            padding: 10px; 
            border-radius: 3px; 
            white-space: pre;
            overflow-x: auto;
            overflow-y: hidden;
            max-width: 100%;
            width: 100%;
            box-sizing: border-box;
        }
    </style>
</head>
<body>
    <h1>TokenSmith Benchmark Results</h1>
    
    <div class="summary">
        <h2>Summary</h2>
        <p><strong>Total Tests:</strong> 6</p>
        <p><strong>Passed:</strong> 6 (100.0%)</p>
        <p><strong>Failed:</strong> 0</p>
        <p><strong>Average Score:</strong> 0.884</p>
        <p><strong>Score Range:</strong> 0.796 - 0.972</p>
        <p><strong>Active Metrics:</strong> keyword, nli, semantic</p>
        
        <h3>Per-Metric Averages</h3>
        <div class="metric-grid">
    <div class="metric-item"><strong>Keyword:</strong> 0.750</div><div class="metric-item"><strong>Nli:</strong> 0.951</div><div class="metric-item"><strong>Semantic:</strong> 0.831</div></div></div><h2>Detailed Results</h2>
    <div class="test-result passed">
        <h3>Explain natural join, theta join, and outer join in relational algebra, and state associativity/commutativity properties relevant to query optimization - <span class="score">PASSED</span></h3>
        <p><strong>Final Score:</strong> <span class="score">0.924</span></p>
        <p><strong>Threshold:</strong> 0.780</p>
        <p><strong>Active Metrics:</strong> semantic, keyword, nli</p>
        
        <div class="metric-grid">
        <div class="metric-item"><strong>Nli Similarity:</strong> 0.997</div><div class="metric-item"><strong>Semantic Similarity:</strong> 0.881</div><div class="metric-item"><strong>Keyword Similarity:</strong> 0.750</div><div class="metric-item"><strong>Keywords Matched:</strong> 3/4</div>
        </div>
        
        <h4>Expected Answer:</h4>
        <pre>Natural join forms a Cartesian product, selects tuples with equal values on common attributes, and removes duplicate attributes, while theta join combines a Cartesian product with a general predicate; both enable combining relations under conditions, and natural join is commutative and associative, which supports join reordering in optimization; outer joins (left, right, full) extend join by padding unmatched tuples with nulls to avoid loss of information and follow join semantics except where unmatched tuples are added with nulls.</pre>
        
        <h4>Retrieved Answer:</h4>
        <pre>- Natural join: take the Cartesian product, select tuples with equality on all common attributes, then remove duplicate attributes.
- Theta join (theta-join): Cartesian product with an arbitrary predicate (e.g., <, >, !=, <=, >=).
- Outer join: left/right/full; include unmatched tuples padded with NULLs.
- Properties: natural join is commutative and associative; enables join reordering.</pre>
    </div>
        
    <div class="test-result passed">
        <h3>How do aggregation with grouping and generalized projection work, and how are nulls treated in selections, joins, projections, set operations, and aggregates - <span class="score">PASSED</span></h3>
        <p><strong>Final Score:</strong> <span class="score">0.880</span></p>
        <p><strong>Threshold:</strong> 0.800</p>
        <p><strong>Active Metrics:</strong> semantic, keyword, nli</p>
        
        <div class="metric-grid">
        <div class="metric-item"><strong>Nli Similarity:</strong> 0.866</div><div class="metric-item"><strong>Semantic Similarity:</strong> 0.835</div><div class="metric-item"><strong>Keyword Similarity:</strong> 1.000</div><div class="metric-item"><strong>Keywords Matched:</strong> 6/6</div>
        </div>
        
        <h4>Expected Answer:</h4>
        <pre>Aggregation partitions tuples by grouping attributes and applies functions like sum, avg, min, max to each group, producing one result per group; generalized projection permits arithmetic expressions and attribute renaming in the projection list; with nulls, selections treat comparisons as unknown and exclude them, joins inherit selection semantics, projections and set operations treat identical tuples with nulls as duplicates, and aggregates ignore nulls in aggregated attributes and return null only when the multiset is empty.</pre>
        
        <h4>Retrieved Answer:</h4>
        <pre>- Aggregation with GROUP BY: partition tuples into groups based on the grouping attributes and apply SUM, AVG, MIN, MAX, COUNT, etc. per group; one output tuple per group.
- Generalized projection: allows arithmetic expressions and attribute renaming in the projection list (for example SELECT A, B, A+B AS total FROM R).
- Nulls in selections: comparisons with NULL evaluate to UNKNOWN and are filtered out by selection predicates.
- Nulls in joins: join predicates behave like selections; if a comparison with NULL is UNKNOWN the tuple does not join.
- Nulls in projections and set operations: duplicate elimination is based on the full set of attribute values, including NULL, so two tuples with NULL in the same positions are considered duplicates.
- Nulls in aggregates: aggregates ignore NULLs in the aggregated attribute; COUNT(*) counts all rows, while COUNT(column) counts only non-NULL values.</pre>
    </div>
        
    <div class="test-result passed">
        <h3>What are the ACID properties of transactions, and how do concurrency control and recovery components enforce them during concurrent execution and failures - <span class="score">PASSED</span></h3>
        <p><strong>Final Score:</strong> <span class="score">0.972</span></p>
        <p><strong>Threshold:</strong> 0.820</p>
        <p><strong>Active Metrics:</strong> semantic, keyword, nli</p>
        
        <div class="metric-grid">
        <div class="metric-item"><strong>Nli Similarity:</strong> 0.998</div><div class="metric-item"><strong>Semantic Similarity:</strong> 0.901</div><div class="metric-item"><strong>Keyword Similarity:</strong> 1.000</div><div class="metric-item"><strong>Keywords Matched:</strong> 8/8</div>
        </div>
        
        <h4>Expected Answer:</h4>
        <pre>Atomicity ensures a transaction's actions are all-or-nothing, enforced by abort/rollback and recovery that can undo partial effects; consistency requires each transaction to preserve database integrity when run alone and relies on the scheduler to admit serializable, recoverable, and preferably cascadeless schedules; isolation makes concurrent executions equivalent to some serial order, commonly achieved with two-phase locking variants that prevent reads of uncommitted data; durability guarantees committed effects persist across crashes via logging to stable storage and redo on restart; together, the transaction manager, concurrency-control (e.g., lock manager under strict 2PL), and recovery manager (logging/checkpoints) provide these guarantees, and in distributed settings coordinators run two-phase commit to atomically commit across sites.</pre>
        
        <h4>Retrieved Answer:</h4>
        <pre>- ACID overview: a transaction should satisfy Atomicity, Consistency, Isolation, and Durability.
- Atomicity: all-or-nothing. If a transaction aborts or the system crashes, none of its partial changes remain. Recovery enforces this using write-ahead logging (WAL) and UNDO of updates from uncommitted transactions.
- Consistency: each transaction, if run alone, takes the database from one consistent state to another by preserving integrity constraints. The scheduler must only allow schedules that are serializable, recoverable, and preferably cascadeless so that concurrent execution does not violate consistency.
- Isolation: concurrent execution of transactions should appear equivalent to some serial order. Concurrency control enforces this, typically via strict two-phase locking (strict 2PL): a lock manager grants shared and exclusive locks, and a transaction holds its locks until commit, preventing reads of uncommitted data and producing serializable, cascadeless schedules.
- Durability: once a transaction commits, its effects must survive crashes. The recovery manager ensures durability by forcing log records to stable storage before commit (WAL) and using REDO during restart to reapply committed updates after a crash.
- Putting it together: the transaction manager coordinates transactions; the concurrency-control component (for example a lock manager implementing strict 2PL) enforces isolation and helps maintain consistency; the recovery manager uses logging, checkpoints, undo, and redo to provide atomicity and durability. In distributed settings, two-phase commit (2PC) can be used so that a group of participants either all commit or all abort.</pre>
    </div>
        
    <div class="test-result passed">
        <h3>How does a B+ tree index organize keys and support search, insert, and delete, and why is it preferred over binary trees for disk-based access - <span class="score">PASSED</span></h3>
        <p><strong>Final Score:</strong> <span class="score">0.796</span></p>
        <p><strong>Threshold:</strong> 0.780</p>
        <p><strong>Active Metrics:</strong> semantic, keyword, nli</p>
        
        <div class="metric-grid">
        <div class="metric-item"><strong>Nli Similarity:</strong> 0.875</div><div class="metric-item"><strong>Semantic Similarity:</strong> 0.875</div><div class="metric-item"><strong>Keyword Similarity:</strong> 0.400</div><div class="metric-item"><strong>Keywords Matched:</strong> 2/5</div>
        </div>
        
        <h4>Expected Answer:</h4>
        <pre>A B+ tree is a balanced, multiway search tree where all keys appear in the leaf level linked for range scans, and internal nodes guide search using separators; search descends from root to a leaf in height proportional to log base fan-out, insert splits full nodes to maintain balance, and delete may redistribute or merge nodes to keep occupancy; high fan-out reduces tree height and I/O, making B+ trees efficient on disk and better than binary trees whose height and I/O would be much larger.</pre>
        
        <h4>Retrieved Answer:</h4>
        <pre>- Structure: A B+ tree is a balanced multiway search tree. Internal nodes store separator keys only (no records). All keys/records are at the leaf level, and leaves are linked for efficient range scans.
- Search: From root to leaf by following separators; height approx log_f(N) where f is fan-out (branching factor). Cost is measured in page I/Os.
- Insert: On overflow, split the full node and promote the median key to the parent; the tree stays balanced.
- Delete: On underflow, merge or redistribute with a sibling to maintain >= ceil(m/2) occupancy.
- Why better than binary trees on disk: large fan-out makes the tree shallow -> far fewer page I/Os. Think log_f(N) pages vs log_2(N) pages for a binary tree; nodes are sized to page/block boundaries and leaf links speed range scans.</pre>
    </div>
        
    <div class="test-result passed">
        <h3>What are functional dependencies and how are they used to achieve BCNF or 3NF through lossless, dependency-preserving decomposition - <span class="score">PASSED</span></h3>
        <p><strong>Final Score:</strong> <span class="score">0.882</span></p>
        <p><strong>Threshold:</strong> 0.700</p>
        <p><strong>Active Metrics:</strong> semantic, keyword, nli</p>
        
        <div class="metric-grid">
        <div class="metric-item"><strong>Nli Similarity:</strong> 0.998</div><div class="metric-item"><strong>Semantic Similarity:</strong> 0.819</div><div class="metric-item"><strong>Keyword Similarity:</strong> 0.600</div><div class="metric-item"><strong>Keywords Matched:</strong> 3/5</div>
        </div>
        
        <h4>Expected Answer:</h4>
        <pre>A functional dependency X -> Y asserts that tuples agreeing on X must agree on Y; BCNF requires every nontrivial FD have a superkey on the left, while 3NF relaxes this by allowing attributes on the right that are part of a key; normalization decomposes a relation into smaller ones such that the join is lossless (typically ensured by a common key/FD condition) and preferably preserves dependencies so that constraints can be enforced without recomputation; BCNF eliminates redundancy more aggressively but may not preserve all FDs, whereas 3NF guarantees dependency preservation with minimal redundancy.</pre>
        
        <h4>Retrieved Answer:</h4>
        <pre>- Functional dependency (FD): X->Y means that whenever two tuples agree on X they must agree on Y. FDs are used (via closure computation) to find keys and superkeys.
- BCNF: a relation schema R is in BCNF if for every nontrivial FD X->Y that holds on R, X is a superkey. This strongly reduces redundancy but may not preserve all FDs.
- 3NF: relaxes BCNF. For every nontrivial FD X->A, either X is a superkey OR A is a prime attribute (appears in some candidate key). 3NF guarantees dependency preservation while controlling redundancy.
- Lossless join: a decomposition R -> {R1,R2,...} is lossless if the join of the decomposed relations reconstructs R without spurious tuples. A sufficient condition is that the common attributes between some pair form a key for one of the components.
- Dependency-preserving decomposition: the union of the FDs projected onto the components implies all original FDs, so constraints can be checked without recomputing joins. BCNF focuses on redundancy removal; 3NF trades a bit more redundancy for guaranteed dependency preservation.</pre>
    </div>
        
    <div class="test-result passed">
        <h3>What isolation guarantees does SQL provide by default, what anomalies can occur at weaker levels, and how do stricter levels prevent them - <span class="score">PASSED</span></h3>
        <p><strong>Final Score:</strong> <span class="score">0.854</span></p>
        <p><strong>Threshold:</strong> 0.700</p>
        <p><strong>Active Metrics:</strong> semantic, keyword, nli</p>
        
        <div class="metric-grid">
        <div class="metric-item"><strong>Nli Similarity:</strong> 0.975</div><div class="metric-item"><strong>Semantic Similarity:</strong> 0.676</div><div class="metric-item"><strong>Keyword Similarity:</strong> 0.750</div><div class="metric-item"><strong>Keywords Matched:</strong> 6/8</div>
        </div>
        
        <h4>Expected Answer:</h4>
        <pre>SQL's serializable level aims for schedules equivalent to some serial execution and prevents phenomena like dirty reads, nonrepeatable reads, and phantoms; weaker levels like read committed prevent dirty reads but allow nonrepeatable reads and phantoms, while repeatable read prevents nonrepeatable reads but may still allow phantoms; strict two-phase locking or predicate locking/index locking can enforce serializability by holding appropriate locks until commit, eliminating these anomalies.</pre>
        
        <h4>Retrieved Answer:</h4>
        <pre>- Levels: READ UNCOMMITTED, READ COMMITTED, REPEATABLE READ, SERIALIZABLE.
- Anomalies: dirty read, nonrepeatable read, phantom.
- Prevention: read committed prevents dirty reads; repeatable read prevents nonrepeatable reads; serializable prevents phantoms (for example via predicate/index locking under strict 2PL).</pre>
    </div>
        </body>
</html>